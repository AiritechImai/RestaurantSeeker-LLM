version: '3.8'

services:
  backend:
    build: ./backend
    ports:
      - "5000:5000"
    environment:
      - LLM_ENDPOINT=http://host.docker.internal:11434/api/generate
      - LLM_MODEL=llama3
    volumes:
      - ./backend:/app
    depends_on:
      - frontend
    networks:
      - bookseeker

  frontend:
    image: nginx:alpine
    ports:
      - "3000:80"
    volumes:
      - ./frontend:/usr/share/nginx/html
    networks:
      - bookseeker

networks:
  bookseeker:
    driver: bridge